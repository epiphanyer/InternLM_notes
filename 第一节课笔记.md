
# 书生·浦语大模型全链路开源体系



## InternLM2分类
按规格分类，有7B和20B两种模型；按使用需求分类，有InternLM2-Base、InternLM2和InternLM2-Chat。

## InternLM2主要亮点
InternLM2在超长上下文、综合性能、对话和创作体验、工具调用能力以及数理能力和数据分析功能等方面都有显著优势。其中，InternLM2-Chat-20B在重点评测上甚至可以达到比肩ChatGPT（GPT3.5）的水平。

## 从模型到应用


### 数据
书生·浦语大模型的数据集名为"书生·万卷"，是一个包含1.6万亿token的多语种高质量数据集，涵盖多种模态和任务。包含文本数据（50 亿个文档，数据量超 1TB），图像-文本数据集（超 2200 万个文件，数据量超 140GB），视频数据（超 1000 个文件，数据量超 900GB）。这个数据集为模型的训练提供了丰富的语言信息和知识基础。

### 预训练
使用了InternLM-Train进行预训练。InternLM-Train是一个基于Transformer架构的预训练模型，它具有1040亿参数，通过在书生·万卷数据集上进行训练，使模型具备了强大的语言理解和生成能力。它支持从 8 卡到千卡训练，千卡训练效率达 92%；无缝接入 HuggingFace 等技术生态，支持各类轻量化技术。

### 微调
开发了XTuner微调框架。XTuner可以根据不同的任务需求，对大模型进行微调，使其在特定领域或任务上表现更加优秀。它具有以下特点：
1. 支持多种任务类型，如增量预训练，指令微调，工具类指令微调
2. 支持全量参数、LoRA、QLoRA 等低成本微调，覆盖各类 SFT 场景
3. 支持多种大语言模型的微调，如 InternLM, Llama, QWen, BaiChuan, ChatGLM，适配多种开源生态，支持加载 HuggingFace、ModelScope 模型或数据集
4. 支持自动优化加速，如 Flash Attention, DeepSpeed ZeRO, Pytorch FSDP，使得开发者无需关注复杂的现存优化与计算加速细节
5. 支持多种硬件，覆盖 NVIDIA 20 系以上所有显卡，最低只需 8GB 现存即可微调 7B 模型
6. 支持多种数据格式，如 Alpaca, MOSS, OpenAI, Guanacao 等等。

### 部署
开发了LMDeploy部署框架。LMDeploy提供大模型在 GPU 上部署的全流程解决方案，包括模型轻量化、推理和服务。可以将大模型快速部署到各种计算平台上，实现模型的实时推理和应用。它具有以下特点：

接口支持：Python, GRPC, RESTful
量化支持：4bit 、8bit 
推理引擎：turbomind, pytorch
服务：openai-server, gradio, triton inference server


### 评测
开发了OpenCompass评测框架，它包含80 套评测集，40 万道题目。OpenCompass可以对模型在多个任务和数据集上的表现进行全面评估，从而了解模型的优势和局限性。它具体包括6 大维度的评测集：

- 学科：初中考试、中国高考、大学考试、语言能力考试、职业资格考试
- 语言：字词释义、成语习语、语义相似、指代消解、翻译
- 知识：知识问答、多语种知识问答
- 理解：阅读理解、内容分析、内容总结
- 推理：因果推理、常识推理、代码推理、数学推理
- 安全：偏见、有害性、公平性、隐私性、真实性、合法性

### 应用
开发了Lagent多模态智能体工具箱和AgentLego多模态智能体工具箱。这些工具箱可以帮助开发者构建和训练多模态智能体，实现图文混合创作、多模态对话等应用场景。



# InternLM2 技术报告

LLM开发的主要阶段：预训练、监督微调和人类反馈强化学习

## 预训练

### 预训练数据
1. 文本数据
2. 代码数据
3. 长文本数据

### 预训练设置
1. tokenization
2. 预训练超参数

### 预训练阶段
1. 4k上下文训练
2. 长上下文训练
3. 特定能力增强训练

## 对齐
### 监督微调SFT
### COOL RLHF
允许奖励模型根据条件特定地将注意力分配给各种偏好。